{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型（Model）与层（Layer）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将模型编写为类，然后在模型调用的地方使用`y_pred = model(x)`的形式进行调用。**模型类**的形式非常简单，主要包含`_init__()`（构造函数，初始化）和`call(input)`（模型调用）两个方法，但也可以根据需要增加自定义的方法。\n",
    "```\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 此处添加初始化代码（包含call方法中会用到的层）\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 此处添加模型调用的代码（处理输入并返回输出）\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们的模型继承了tf.kears.Model。继承tf.keras.Model的一个好处在于我们可以使用父类的若干方法和属性，例如在实例化类后可以通过model.variables这一属性直接获得模型中的所有变量，免得我们一个个显式指定变量的麻烦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时，我们引入“层”（Layer）的概念，层可以视为比模型粒度更细的组件单位，将计算流程进行了封装。我们可以使用层来快速搭建模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred = tf.matmul(X, w) + b`可以通过模型类的方式编写如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(units=1, kernel_initializer=tf.zeros_initializer(),\n",
    "                                           bias_initializer=tf.zeros_initializer())\n",
    "    \n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'linear_4/dense_4/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[0.40784496],\n",
      "       [1.191065  ],\n",
      "       [1.9742855 ]], dtype=float32)>, <tf.Variable 'linear_4/dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([0.78322077], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# 实例化Linear类\n",
    "model = Linear()\n",
    "# 声明一个梯度下降优化器\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "for i in range(100):\n",
    "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 调用模型\n",
    "        y_pred = model(X)\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    # 自动计算自变量的梯度\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    # 自动根据梯度更新参数\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "print(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们没有显式地声明`w`和`b`两个变量并写出`y_pred = tf.matmul(X, w) + b`这一线性变换，而是在初始化部分实例化了一个全连接层（`tf.keras.layers.Dense`），并在call方法中对这个层进行调用。全连接层封装了`output = activation(tf.matmul(input, kernel) + bias)`这一线性变换+激活函数的计算操作，以及`kernel`和`bias`两个变量。当不指定激活函数时（即`activaion(x) = x`），这个全连接层就等价于我们上述的线性变换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础示例：多层感知机（MLP）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们从编写一个最简单的多层感知机（Multilayer Perceptron，MLP）开始，介绍TensorFlow的模型编写方式。这里我们使用多层感知机完成MNIST手写体数字图片数据集的分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先进行预备工作，实现一个简单的`DataLoader`类来读取MNIST数据集数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.contrib.learn.datasets.load_dataset('mnist')\n",
    "        self.train_data = mnist.train.images                               # np.array [55000, 784] \n",
    "        self.train_labels = np.asarray(mnist.train.labels, dtype=np.int32)  # np.array [55000] of int32\n",
    "        self.eval_data = mnist.test.images                                  # np.array [10000, 784]\n",
    "        self.eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)    # np.array [10000] of int32\n",
    "        \n",
    "    def get_batch(self, batch_size):\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多层感知机的模型类实现与上面的线性模型类似，所不同的地方在于层数增加了（顾名思义，“多层”感知机），以及引入了非线性激活函数（这里使用了ReLU函数，即下方的`activation=tf.nn.relu`）。该模型输入一个向量（比如这里是拉直的1×784手写体数字图片），输出10维的信号，分别代表这张图片属于0到9的概率。这里我们加入了一个predict方法，对图片对应的数字进行预测。在预测的时候，选择概率最大的数字进行预测输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**线性整流函数**（Rectified Linear Unit，**ReLU**）,又称修正线性单元, 是一种人工神经网络中常用的激活函数（activation function），通常指代以斜坡函数及其变种为代表的非线性函数。  \n",
    "比较常用的线性整流函数有斜坡函数$f(x)=max(0,x)$，以及带泄露整流函数 (Leaky ReLU)。  \n",
    "$f(x)=max(0,x)$又可写成$f(x)=\\begin{cases}x& \\text{x}{\\geq}{0;}\\\\0& \\text{x}{\\lt}{0}\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        y = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        logits = self(inputs)\n",
    "        return tf.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一些模型超参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化模型，数据读取类和优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "data_loader = DataLoader()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后迭代进行以下步骤：\n",
    "* 从DataLoader中随机取一批训练数据；\n",
    "* 将这批数据送入模型，计算出模型的预测值；\n",
    "* 将模型预测值与真实值进行比较，计算损失函数（loss）；\n",
    "* 计算损失函数关于模型变量的导数；\n",
    "* 使用优化器更新模型参数以最小化损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 4.704428\n",
      "batch 9: loss 4.003401\n",
      "batch 18: loss 3.584399\n",
      "batch 27: loss 2.823752\n",
      "batch 36: loss 2.561762\n",
      "batch 45: loss 2.677138\n",
      "batch 54: loss 1.891707\n",
      "batch 63: loss 2.126160\n",
      "batch 72: loss 1.846991\n",
      "batch 81: loss 1.849578\n",
      "batch 90: loss 1.490038\n",
      "batch 99: loss 1.626411\n",
      "batch 108: loss 1.321092\n",
      "batch 117: loss 1.195409\n",
      "batch 126: loss 2.006490\n",
      "batch 135: loss 2.017419\n",
      "batch 144: loss 1.338521\n",
      "batch 153: loss 1.215853\n",
      "batch 162: loss 1.122013\n",
      "batch 171: loss 1.595369\n",
      "batch 180: loss 1.038400\n",
      "batch 189: loss 1.428598\n",
      "batch 198: loss 1.561793\n",
      "batch 207: loss 1.073895\n",
      "batch 216: loss 1.192201\n",
      "batch 225: loss 0.984081\n",
      "batch 234: loss 1.609973\n",
      "batch 243: loss 1.095677\n",
      "batch 252: loss 1.887974\n",
      "batch 261: loss 0.893708\n",
      "batch 270: loss 1.444066\n",
      "batch 279: loss 1.179849\n",
      "batch 288: loss 1.277989\n",
      "batch 297: loss 0.976289\n",
      "batch 306: loss 1.266387\n",
      "batch 315: loss 1.156502\n",
      "batch 324: loss 1.504180\n",
      "batch 333: loss 1.128154\n",
      "batch 342: loss 0.899152\n",
      "batch 351: loss 1.213965\n",
      "batch 360: loss 1.117081\n",
      "batch 369: loss 1.306384\n",
      "batch 378: loss 1.522684\n",
      "batch 387: loss 0.850099\n",
      "batch 396: loss 0.707890\n",
      "batch 405: loss 0.758045\n",
      "batch 414: loss 0.811644\n",
      "batch 423: loss 1.202524\n",
      "batch 432: loss 1.098900\n",
      "batch 441: loss 0.921399\n",
      "batch 450: loss 0.933994\n",
      "batch 459: loss 0.724710\n",
      "batch 468: loss 0.497148\n",
      "batch 477: loss 1.039911\n",
      "batch 486: loss 0.792317\n",
      "batch 495: loss 0.647077\n",
      "batch 504: loss 0.774216\n",
      "batch 513: loss 0.916208\n",
      "batch 522: loss 0.672440\n",
      "batch 531: loss 0.735257\n",
      "batch 540: loss 0.899366\n",
      "batch 549: loss 0.641931\n",
      "batch 558: loss 0.703449\n",
      "batch 567: loss 0.897423\n",
      "batch 576: loss 0.932883\n",
      "batch 585: loss 0.954510\n",
      "batch 594: loss 0.860244\n",
      "batch 603: loss 0.784890\n",
      "batch 612: loss 1.032313\n",
      "batch 621: loss 1.258023\n",
      "batch 630: loss 0.501466\n",
      "batch 639: loss 1.199296\n",
      "batch 648: loss 1.371184\n",
      "batch 657: loss 0.882296\n",
      "batch 666: loss 0.834440\n",
      "batch 675: loss 0.957902\n",
      "batch 684: loss 1.077143\n",
      "batch 693: loss 0.548516\n",
      "batch 702: loss 1.117426\n",
      "batch 711: loss 0.819122\n",
      "batch 720: loss 0.980079\n",
      "batch 729: loss 0.433698\n",
      "batch 738: loss 0.835201\n",
      "batch 747: loss 0.664412\n",
      "batch 756: loss 0.769406\n",
      "batch 765: loss 1.285105\n",
      "batch 774: loss 0.995272\n",
      "batch 783: loss 0.920137\n",
      "batch 792: loss 0.956150\n",
      "batch 801: loss 0.765756\n",
      "batch 810: loss 1.036881\n",
      "batch 819: loss 0.802200\n",
      "batch 828: loss 0.731620\n",
      "batch 837: loss 0.828760\n",
      "batch 846: loss 1.094940\n",
      "batch 855: loss 0.593319\n",
      "batch 864: loss 0.609304\n",
      "batch 873: loss 1.026094\n",
      "batch 882: loss 1.196567\n",
      "batch 891: loss 0.931383\n",
      "batch 900: loss 0.889037\n",
      "batch 909: loss 1.396260\n",
      "batch 918: loss 0.931684\n",
      "batch 927: loss 0.786204\n",
      "batch 936: loss 0.603222\n",
      "batch 945: loss 0.981007\n",
      "batch 954: loss 0.793526\n",
      "batch 963: loss 0.604423\n",
      "batch 972: loss 1.022755\n",
      "batch 981: loss 0.712700\n",
      "batch 990: loss 0.737504\n",
      "batch 999: loss 0.890420\n"
     ]
    }
   ],
   "source": [
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_logit_pred = model(tf.convert_to_tensor(X))\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_logit_pred)\n",
    "        if batch_index % 9 == 0:\n",
    "            print('batch %d: loss %f' % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们使用验证集测试模型性能。具体而言，比较验证集上模型预测的结果与真是结果，输出预测正确的样本数占总样本数的比例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.823000\n"
     ]
    }
   ],
   "source": [
    "num_eval_samples = np.shape(data_loader.eval_labels)[0]\n",
    "y_pred = model.predict(data_loader.eval_data).numpy()\n",
    "print('test accuracy: %f' % (sum(y_pred == data_loader.eval_labels) / num_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络（CNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络（Convolutional Neural Network，CNN）是一种结构类似于人类或动物的视觉系统的人工神经网络，包含一个或多个卷积层（Convolutional Layer）、池化层（Pooling Layer）和全连接层（Dense Layer）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,  # 卷积核数目\n",
    "            kernel_size=[5, 5],  # 感受野大小\n",
    "            padding='same',  # padding策略\n",
    "            activation=tf.nn.relu  # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "        x = self.conv1(inputs)  # [batch_size, 28, 28,32]\n",
    "        x = self.pool1(x)  # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)  # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)  # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)  # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)  # [batch_size, 1024]\n",
    "        x = self.dense2(x)  # [batch_size, 10]\n",
    "        return x\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        logits = self(inputs)\n",
    "        return tf.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将前面的`model = MLP()`更换成`model = CNN()`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "model = CNN()\n",
    "data_loader = DataLoader()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.296304\n",
      "batch 111: loss 0.080120\n",
      "batch 222: loss 0.124463\n",
      "batch 333: loss 0.020256\n",
      "batch 444: loss 0.063452\n",
      "batch 555: loss 0.232355\n",
      "batch 666: loss 0.018755\n",
      "batch 777: loss 0.119675\n",
      "batch 888: loss 0.019599\n",
      "batch 999: loss 0.136880\n"
     ]
    }
   ],
   "source": [
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_logit_pred = model(tf.convert_to_tensor(X))\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_logit_pred)\n",
    "        if batch_index % 111 == 0:\n",
    "            print('batch %d: loss %f' % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.982900\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(data_loader.eval_data).numpy()\n",
    "print('test accuracy: %f' % (sum(y_pred == data_loader.eval_labels) / num_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 循环神经网络（RNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环神经网络（Recurrent Neural，RNN）是一种适宜于处理序列数据的神经网络，被广泛用于语言模型、文本生成、机器翻译等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们使用RNN来进行尼采风格文本的自动生成。  \n",
    "这个任务的本质其实预测一段英文文本的接续字母的概率分布。比如，我们有以下句子：\n",
    "```\n",
    "I am a studen```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个句子（序列）一共有13个字符（包含空格）。当我们阅读到这个由13个字符组成的序列后，根据我们的经验，我们可以预测出下一个字符很大概率是“t”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
